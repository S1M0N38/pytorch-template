# Number of epochs to train for.
# Default: 0 (i.e. do not train).
epochs = 5

# Validate model and log metrics on TensorBoard every x epochs.
# Default: epochs (i.e. validate only on last epoch).
validate = 1

# Log training metrics on TensorBoard every x steps.
# A good value is int(sqrt(baches_per_epoch)).
# Default: steps (i.e. log only on last step).
log = 8

# Where to save training artefacts (logs, checkpoints, etc.).
# Default: '.' (i.e. save in the current working directory).
path = "experiments/MNIST/"

# Number of GPUs to use (0, 1, 2, ...).
# Default: 0 (i.e. use CPU).
num_gpus = 0

# [patience]
# Patience values, number of steps before early stopping is triggered.
# 0 means immidiatly stop if value is getting worst.
# You can use validation loss/metrics
# Default: loss = epochs (i.e. never trigger early stopping )
# Default: your-own-metrics = epochs (i.e. never trigger early stopping)
# loss = 10
# accuracy-top-1 = 3
# accuracy-top-5 = 5


[model]
# Model impoted from `models` module.
class = "LeNet5"
num_classes = 10

[loss]
# Loss imported from `losses` module.
class = "CrossEntropyLoss"

[optimizer]
# Optimizer imported from `torch.optim` module.
# Not interested in exploring with custom optimizers.
class = "Adam"
lr = 1e-3

[lr_scheduler]
# lr_scheduler imported from `torch.optim.lr_scheduler` module.
# Not interested in exploring with custom lr_scheduler.
class = "CosineAnnealingLR"
T_max = 5
eta_min = 1e-5

[dataloaders.train]
# Dataloader imported from `dataloaders` module.
# `dataloaders.train` is the dataloader used for training.
class = "MNISTTrainDataLoader"
path = "datasets/MNIST" # where to find/download the dataset
batch_size = 512
num_workers = 2

[dataloaders.val]
# Dataloader imported from `dataloaders` module.
# `dataloaders.val` is the dataloader used for validation during training.
class = "MNISTValDataLoader"
path = "datasets/MNIST"
batch_size = 512
num_workers = 2

[dataloaders.test]
# Dataloader imported from `dataloaders` module.
# `dataloaders.test` is the dataloader used for testing after the training.
# Sometimes the test datasets is the same as the validation dataset.
class = "MNISTTestDataLoader"
path = "datasets/MNIST"
batch_size = 512
num_workers = 2

[metrics.train.accuracy-top-1]
# Metric imported from `metrics` module.
# `metrics.train` are obtaing from `dataloaders.train` dataloader,
# and are logged during training. Do not use as proxy for model's performance.
class = "Accuracy"
task = "multiclass"
num_classes = 10
top_k = 1

[metrics.val.accuracy-top-1]
# Metric imported from `metrics` module.
# `metrics.val` are obtaing from `dataloaders.val` dataloader,
# and are logged during training. Can be use as proxy for model's performance.
class = "Accuracy"
task = "multiclass"
num_classes = 10
top_k = 1

[metrics.val.accuracy-top-5]
# Metric imported from `metrics` module.
# `metrics.val` are obtaing from `dataloaders.val` dataloader,
# and are logged during training. Can be use as proxy for model's performance.
class = "Accuracy"
task = "multiclass"
num_classes = 10
top_k = 5

[metrics.test.accuracy-top-1]
# Metric imported from `metrics` module.
# `metrics.test` are obtaing from `dataloaders.test` dataloader.
# Are the best metrics for extimating model's performance.
class = "Accuracy"
task = "multiclass"
num_classes = 10
top_k = 1
